<!DOCTYPE html><html><head><title>Starphleet Documentation</title><link rel="stylesheet" href="main.css"><link rel="stylesheet" href="javascript/highlight.js/styles/monokai.css"><script src="javascript/jquery.min.js"></script><script src="javascript/bootstrap.min.js"></script><script src="javascript/toc.min.js"></script><script src="javascript/highlight.js/highlight.pack.js"></script><script src="javascript/main.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"></head><body><nav class="navbar navbar-default navbar-fixed-top"><header class="navbar-header"><a href="index.html" class="navbar-brand">Starphleet</a><ul class="nav navbar-nav"><li><a href="https://github.com/wballard/starphleet">Source</a></li></ul></header></nav><nav id="leftnav" class="leftnav"><section id="toc" class="sidebar-nav sidebar-nav-fixed"></section></nav><article id="main" class="main"><p><h1>Starphleet</h1></p>
<div class="well">
The fully open container based continuous deployment PaaS
</div>

<p>This is a toolkit for turning virtual or physical machine infrastructure
into a continuous deployment stack. Here are some of the observed
problems in autodeployment, and how Starphleet solves them:</p>
<ul>
<li>Virtualization wastes resources, specifically RAM and CPU running
multiple operating system images, which costs real money<ul>
<li>containerization is the new virtualization, using LXC</li>
</ul>
</li>
<li>PaaS has the same vendor lock-in risks of old proprietary software<ul>
<li>full open source is the only way to go</li>
<li>allow installation on public and private clouds, as well as
computers</li>
</ul>
</li>
<li>Continous deployment is too hard, so folks default to batches and
sprints<ul>
<li>leverage git, allowing deployment with no more than the normal <code>git
push</code> you use to share code</li>
<li>make continuous deployment the default</li>
<li>provides drainstop, restart, failover, and rollback as built ins</li>
<li>be like unix: files, scripts, environment variables -- and spare
folks from the learning curve of yet another system tool</li>
<li>platform package managers, <code>npm</code>, <code>gem</code>, <code>apt</code> beat learning a new
package/script system, and there is a ton of resource available</li>
<li>Heroku Buildpacks already exist for most platforms, use them</li>
<li>Focus on building services, not on systems</li>
</ul>
</li>
<li>Multiple machine deployment is more work than running locally<ul>
<li>Make load balancing the default, spanning computers and geographies</li>
<li>Make commands run across a phleet by default</li>
</ul>
</li>
<li>Making many small services is hard to deploy<ul>
<li>Containerizing allows multiple services to benefit from failover and
redundancy without burning two machines/VMs per service</li>
<li>Allow multiple services to be mounted behing one HTTP endpoint and
avoid cross domain and CORS hell</li>
</ul>
</li>
<li>Seeing what is going on across multiple machines is hard<ul>
<li>aggregate all the logs for each container ship in the phleet, and for
the entire phleet</li>
<li>provide simple dashboards that give you the basics without requiring
understanding and installing other monitoring software</li>
</ul>
</li>
<li>Autodeployment systems all seem to have their own system which itself
needs to be deployed!<ul>
<li>Starphleet leverages git heavily, avoiding the need for yet another
database or daemon</li>
<li>Images are used as a starting point, avoiding the need to <em>install</em>
the install software</li>
</ul>
</li>
</ul>
<h1>Concepts</h1>
<p>The grand tour so you know what we are talking about with all of our
silly names.</p>
<h2>The 12-Factor App</h2>
<p>Starphleet owes a lot to the <a href="http://12factor.net">12 factor app</a>. Learn
about it.</p>
<h2>Command Line</h2>
<p>All of starphleet is available from a simple command line script, which
lets you hook it into any existing scripting framework you like without
needing to learn an API. The command line program is mostly shell, and
some nodejs, which is a nice way to deploy to your laptop or personal
computer to manage starphleet.</p>
<pre><code class="language-bash">npm install &quot;git+https://github.com/wballard/starphleet.git&quot;
starphleet --help</code></pre>
<h2>Phleet</h2>
<p>The top level grouping. You manage starphleet at this level. You can
make as many phleets as you like to arrange different groupings.</p>
<h2>Headquarters</h2>
<p>A git repository that instructs a phleet how to operate. Using git in
this way gives a versioned database of your configuation, allows you to
edit and work with your own tools, and allows multiple hosting options.</p>
<h3>Security</h3>
<p><strong>No Fooling Important</strong> -- your <code>&lt;headquarters_url&gt;</code> git repo needs to be
reachable by each ship running the starphleet software.</p>
<p>The simple thing to do is use a public git repository via <code>https</code>. If you
really need a private repository, or security, you can specify a private
key and access git via <code>git+ssh</code>.</p>
<p>And, in the cases when you need it, you can host your phleet and
headquarters entierly inside your own firewall.</p>
<h2>Ship</h2>
<p>Our cute name for a host computer or virtual machine, it goes with the
phleet metaphor. There are many ships in a fleet to provide scale and
geographic distribution.</p>
<h2>Orders</h2>
<p>An individual program, run in a container, supported by a buildpack, on
a ship, autodeployed across a phleet. Services provide your application
functionality over HTTP, including the use of server sent events and
websockets.</p>
<h1>Phleets</h1>
<p>Check the main <a href="https://github.com/wballard/starphleet">readme</a>. In
particular pay attention to the environment variables for public and
private keys.</p>
<h1>Headquarters</h1>
<p>A headquarters instructs a phleet with orders to deploy and how to serve
them. A headquarters is a git repository, and hosted in a location on
the internet where each ship in the phleet can reach it.  The easiest
thing to do is host on <a href="http://www.github.com">github</a> or
<a href="http://www.bitbucket.com">bitbucket</a>.</p>
<p>The simplest possible phleet has a directory structure like:</p>
<pre><code>..
.
orders</code></pre>
<p>Which will serve exactly one service at <code>/</code>. The path structure of the
headquarters create the virtual HTTP patch structure of your services,
specifically to let you have a set of services, implemented in different
technologies, to be federated together behind on domain name. This is
particularly useful for single page applications making use of a set of
small, sharp back end services.</p>
<p>As an example, imagine an application that has a front end, and two back
end web services <code>workflow</code> and <code>users</code>.</p>
<pre><code>.
..
orders
workflow/
  .
  ..
  orders
users/
  .
  ..
  orders</code></pre>
<h2>Orders</h2>
<p>An <code>orders</code> file is simply a shell script run in the context of
starphleet, at a given path, to control the autodeployment of a service.
You can put anything in the script you like, it is just a shell script
after all, but in practice there are only two things to do:</p>
<pre><code class="language-bash">export PORT
autodeloy &lt;git_url&gt;</code></pre>
<p>Setting up orders as a shell script is to allow your creativity to run
wild, but without you needing to learn a custom tool, DSL, scripting
language, config database, or API.</p>
<h3>Security</h3>
<p><strong>No Fooling Important</strong>, <code>&lt;git_url&gt;</code>, just like your <code>&lt;headquarters_url&gt;</code>
needs to be reachable from each ship in the fleet.</p>
<h3>Branches and Versions</h3>
<p>You can specify your <code>&lt;git_url&gt;</code> like <code>&lt;git_url&gt;#&lt;branch&gt;</code>, where branch can
be a branch, a tag, or a commit sha -- anything you can check out. This
hashtag approach lets you specify a deployment branch, as well as pin
services to specific versions when needed.</p>
<h1>Environments</h1>
<p>Your app will need to talk to things: external web services,
storage-as-a-service, databases, you name it. Starphleet goes back to
basics and lets you set these through environment variables.</p>
<p>Some environment variables are just config, and some environment
variables are really secrets, so starphleet provides multiple locations
where you can keep variables, with different security thoughts.</p>
<p>The environment variables are sourced in the order listed below, which
allows you to override.</p>
<h2>Environment Variables</h2>
<table>
<thead>
<tr>
<th>Name</th>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>PORT</td>
<td>number</td>
<td>This is an all important environment variable, and it is expected your service will honor it, publishing traffic here. This <code>PORT</code> is used to know where to connect the ship&#39;s proxy to your individual service.</td>
</tr>
<tr>
<td>autodeploy</td>
<td>&lt;git_url&gt;</td>
<td>This command in orders tells starphleet where to grab code from git.  While it is possible to put this globally, you really should limit it just to <code>orders</code> files.</td>
</tr>
<tr>
<td>STARPHLEET_BASE</td>
<td>name</td>
<td>This is the named base container to use for a service, which you can use to speed up builds by having common packages pre-installed.  If not specified, this defaults to <code>starphleet-base</code> which is a general purpose buildpack aware container.</td>
</tr>
<tr>
<td>BUILDPACK_URL</td>
<td>&lt;git_url&gt;</td>
<td>Set this to use a custom buildpack, otherwise Starphleet with attempt to use its default buildpacks to autodetect.</td>
</tr>
<tr>
<td>PUSH_HEADQUARTERS</td>
<td>any</td>
<td>When set, this will cause your headquarters to push back to the origin repository, allowing you to share state between ships.</td>
</tr>
</tbody>
</table>
<h2>.env</h2>
<p>Services themselves can have variables, these are inspired by Heroku,
and you keep them in the source repository of each service. These are
the variables with the lowest precedence.</p>
<p>This is where you specify a <code>BUILDPACK_URL</code>, but you can also put in
other variables as you see fit.</p>
<p>Your services will often be hosted in public repositories, so the config
you put in here should be about development mode or public settings.</p>
<h2>orders</h2>
<p>The <code>orders</code> file itself is sourced for your service. This is where a
service learns about <code>PORT</code> and <code>autodeploy</code>.</p>
<p>These settings are laid over the service, and provide the ability to set
variables for a service in the context of a single phleet, compared to
the service variables which are truly generic.</p>
<h2>.starphleet</h2>
<p>Starphleet wide environment variables are applied last, leading to the
highest precedence. This is a great place to have your production
usernames, passwords, and connection strings.</p>
<p>Different than most systems, Starphleet sticks with the git/files
metaphor even for this configuration, rather than a command line to
set/get variables. All the benefits of source control and using your own
tools, and no additional server software is needed, making starphleet
simpler and less to break.</p>
<p>As an example:</p>
<pre><code class="language-bash">#all services will see this domain name
export DOMAIN_NAME=&quot;production.com&quot;
#every service is told to run at 3000 inside its container
export PORT=3000</code></pre>
<p>Now, this is a file right in your headquarters. To keep these private
you put your headquarters in a private, hidden repository than can only
be reached by private key <code>git+ssh</code>.</p>
<h2>healthcheck</h2>
<p>Each service repository can supply a <code>healthcheck</code> file, which contains
an URL snippet <code>http://&lt;container-ip&gt;:&lt;container-port&gt;/&lt;snippet&gt;</code>. You
supply the <code>&lt;snippet&gt;</code>, and if you don&#39;t provide it, the default is just
blank, meaning hitting the root of your service.</p>
<p>As soon as a 200 comes back, you are good to go and the new service is
put into rotation to take over future requests from the prior version.</p>
<p>You get 60 seconds for your service to return this 200 past when it is
initially started.</p>
<h1>SSH Access</h1>
<p>A big difference form other PaaS: the ships are yours, and you can <code>ssh</code>
to them. Specifically, you can put as many public keys in the
<code>authorized_keys</code> folder of your headquarters, one per file, to let in
other users as you see fit via ssh.</p>
<p>These special users are called <code>admirals</code>, again sticking with our
nautical theme.</p>
<p>Users get to a ship with <code>ssh admiral@ship</code>. The admiral account is a
member of <code>sudoers</code>.</p>
<p>In practice, this open access to the base machine lets you do what you
want, when you want, truly open. And if you use this power to somehow
destroy a ship -- somebody has to wreck the ship -- you can always just
add a new one.</p>
<p>And, even more fun -- the <code>authorized_keys</code> themselves are continuously
deployed, just add and remove admirals by adding and removing public key
files in your github repository. Updates in seconds.</p>
<h1>Containers</h1>
<p>Starphleet encapsualtes each service in an LXC container. Starting from
a base container equipped with Heroku buildpacks, you can create your
own custom containers to speed up builds as needed, as well as your own
custom buildpacks.</p>
<p>Containers serve as the bas system image, and are then customized with a
servive+buildpack. In general:</p>
<ul>
<li>containers serve to create fixed, cached sets of software such as compilers</li>
<li>buildpacks exist to install dynamic software, for example referenced
<code>npm</code> packages.</li>
</ul>
<h2>Self Healing</h2>
<p>Each ship uses a pull strategy to keep the headquarters up to date. This
is different than other platforms where you <em>push</em> your software to
deploy. Some folks will not like this, as it involves polling. Some
folks think polling is evil. Noted. Here are the reasons:</p>
<ul>
<li>Ships go up and down, pull based lets ships catch up easily if they
happened to be down when a new version was released</li>
<li>Adding new ships is simple, just <code>starphleet add ship</code>, the pull
mechanism catches it up automatically</li>
<li>You don&#39;t have to personally sit through a Heroku style push, watching
the build go by -- you can move on to the next feature</li>
</ul>
<h2>Rolling Updates</h2>
<p>As new versions of services are updated, fresh containers are built and
run in parallel to prior versions with a drainstop. This means in
process requests aren&#39;t interrupted like on other platforms.</p>
<p>OK -- so this is a bit idealistic. Lots of folks program in a database
heavy way with no real notion of backward compatibility. Getting the
full benefit of autodeployment and rolling upgrades requires you to
think about your storage, and how different versions of code may
interact with that. Or, totally ingore it -- you won&#39;t be any worse off
that with other autodeploy systems, or classic &#39;off the air&#39; deployment.</p>
<h2>Base Containers</h2>
<p>Base containers provide system software and are used via file system
snapshot, so provisioning them is quick. This serves as a base layer to
give you system control.</p>
<h2>Scripts</h2>
<p>Given any script in your headquarters named <code>containers/name</code>, an LXC
container named <code>name</code> will be created for you, and automatically
updated any time the script changes.</p>
<p>These custom build scripts are run as virtual root in a dedicated LXC
container that is itself a snapshot built on top of starphleet&#39;s own
base container.</p>
<p>This is similar in a way to a <code>Dockerfile</code>, but in keeping with the
starphleet design, is just a shellscript, so there are no new custom
commands to learn.</p>
<h2>Buildpacks</h2>
<p>Buildpacks autodetect and provision services on containers for you
without worrying about system or os level setup.</p>
<p>Huge thanks to Heroku for having open buildpacks, and to the open source
community for making and extending them. The trick that makes the
starphleet orders file so simple is the use of buildpacks and platform
package managers to get your dependencies running.</p>
<h2>Provided Buildpacks</h2>
<p>Using the available Heroku buildpacks, out of the box starphleet with
autodetect and provision a service running:</p>
<ul>
<li>Ruby</li>
<li>NodeJS</li>
<li>Java</li>
<li>Play</li>
<li>Python</li>
<li>PHP</li>
<li>Clojure</li>
<li>Go</li>
<li>Perl</li>
<li>Scala</li>
<li>Dart</li>
<li>NGINX static</li>
<li>Apache</li>
</ul>
<h1>Services</h1>
<p>Services are any program you can dream up that meet these conditions:</p>
<ul>
<li>Serve HTTP traffic to a PORT</li>
<li>Are hosted in git</li>
<li>Can read environment variables to get their settings</li>
<li>Have a <em>buildpack</em> to get dependencies</li>
</ul>
<p>Unlike other PaaS which is trying to force you into a specific notion of
scalable programming, starphleet gives you more freedom.</p>
<ul>
<li>No specific scale up / scale out tradeoff is enforced</li>
<li>No specific &#39;scaleable database&#39; is mandated</li>
<li>This is no specific API</li>
<li>There are no mandated programming languages</li>
</ul>
<p>Services are run in LXC containers, and as such don&#39;t have acess to the
entire machine, they are root in their own world of a container. This is
convenient, particularly when making custom buildpacks as you can just
use <code>apt-get install</code> without a sudo.</p>
<p>Containers are thrown away often, on each new version, and each server
reboot. So, while you do have local filesystem access inside a container
running a service, don&#39;t count on it living any lenght of time.</p>
<h2>Autodeploy</h2>
<p>This is really easy. Just commit and push to the repository referenced
in the orders. Every ship will get it.</p>
<h2>Rollback</h2>
<p>Again, this is really easy, just use <code>git revert</code> and pull out commits,
then push to the repository referenced in the orders. Best thing is,
this preserves history.</p>
<h2>Testing</h2>
<p>Check the main <a href="https://github.com/wballard/starphleet">readme</a>.</p>
<h1>Ships</h1>
<p>Each ship in the phleet runs every ordered service. This makes things
nice and symmetrical, and simplifies scaling. Just add more ships if you
need more capacity. If you need full tilt performance, you can easily
make a phleet with just one ordered service at <code>/</code>. Need a different
mixture of services? Launch another phleet!</p>
<h2>Linux Versions</h2>
<p>The actual ships are provided as virual machine images in EC2, VMWare,
and VirtualBox format. To keep things simple, these images are
standardized on a single Linux version. Some folks who have varying
preferences or notions about OS support contracts may not like this.
Noted. All of starphleet is open, feel free to port it over anywhere you
like.</p>
<p>In practice, packing things up as orders with buildpacks saves you from
OS-ing around ships and just lets you focus on writing your services.
Think a bit like Heroku, where the version of the OS is a decision made
for you to save time.</p>
<h2>EC2 Instance Sizes</h2>
<p>Please, don&#39;t cheap out and go to small. The recommended minimum size is an
m2.xlarge -- which is roughly the power of a decent laptop, so this is
the default. You can change this with <code>EC2_INSTANCE_SIZE</code>.</p>
<h1>Phleets</h1>
<p>Don&#39;t feel limited to just one phleet. Part of making your own PaaS is
to give you the freedom to mix and match as you see fit.</p>
<h2>Log Aggregation</h2>
<p>Just nearly as painful as getting everything deployed -- keeping an eye
on it. You can generally get software that watches CPU, RAM, Disk,
Network interfaces -- and it basically tells you nothing about your
application. The real value is in logging, where you can have
specifics and details to hunt down trouble.</p>
<p>But, logs across multiple services, multiple machines, and multiple
geographies aren&#39;t exactly fun, so starphleet aggregates all logs from
all ships and containers for you. From there you can pipe this into
services like <a href="http://www.splunk.com">splunk</a>, or my personal favorite, tail it
and grep it.</p>
<p>This also keeps you disks from filling up, since the logs are written to
a stream and not to disk.</p>
<h2>Geo Scaling AWS</h2>
<p>By default, starphleet sets up four zones, three US, one Europe. Ships
are added explicitly to zones, and you aren&#39;t required to use them all.
It&#39;s OK for you to set up just in one location if you like. Or even have
a phleet with one ship.</p>
<footer><p>Copyright &copy 2013</p></footer></article></body></html>