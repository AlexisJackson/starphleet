#!/usr/bin/env starphleet-launcher
### Script to take log files and push them up to S3 for later reporting
### Usage:
###   archive_log_to_s3 <path_to_logfile> nginx
# Setup from logrotate config 'lastaction' block. 
## lastaction
##  archive_log_to_s3 $1 nginx
## endscript

# Requires configuration of AWS creds which needs to include:
export AWS_ACCESS_KEY_ID=${PHLEET_LOGS_ARCHIVE_AWS_ACCESS_KEY_ID}
export AWS_SECRET_ACCESS_KEY=${PHLEET_LOGS_ARCHIVE_AWS_SECRET_ACCESS_KEY}
export AWS_DEFAULT_REGION=${PHLEET_LOGS_ARCHIVE_AWS_DEFAULT_REGION:-us-west-2}
# this serves as an optional server (ship) level setting that can be used to further split the
# logs up into groups like, staging, production, prodiction-ship1, etc.
export LOG_ARCHIVE_PREFIX=${PHLEET_LOGS_ARCHIVE_PREFIX}

# If aws credential are not setup, or there is an AWS failure, the action fails but does not affect the log rotate action.

# Spaces are not allowed in the inputs
original_file=$(echo "$1" | sed 's/[[:space:]]//g')
category=$(echo "$2" | sed 's/[[:space:]]//g')

compressed_file="$original_file.1.gz"
echo "Archive $compressed_file"

# Create a unique file name to store up to s3 to include category+hostname+timestamp
extension="$(hostname)_$(date -u "+%m-%d-%Y_%H:%M:%S")_$AWS_REGION.log.gz"
cp_to_file="$category.$extension"

if [ -n "${LOG_ARCHIVE_PREFIX}" ]; then
  aws s3 cp $compressed_file s3://phleet-logs/${LOG_ARCHIVE_PREFIX}/$category/$cp_to_file &>> /var/log/archive_to_s3_for_$category.log
else
  aws s3 cp $compressed_file s3://phleet-logs/$category/$cp_to_file &>> /var/log/archive_to_s3_for_$category.log
fi

